{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea990493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "from torch.nn import functional as F\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1da5b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tea(nn.Module, PyTorchModelHubMixin, \n",
    "          repo_url=\"tea\", \n",
    "          license=\"mit\"):\n",
    "    \"\"\"\n",
    "    The embedding-based Alphabet (tea) model for converting input pLMs embeddings into sequences.\n",
    "\n",
    "    This model consists of two linear layers, normalization, and dropout,\n",
    "    followed by a codebook-based decoder. It provides methods to compute Shannon \n",
    "    entropy over the output distribution and to convert model outputs into character \n",
    "    sequences.\n",
    "\n",
    "    Args:\n",
    "        representation_size (int): Dimensionality of input representations.\n",
    "        hidden_size (int): Hidden size for first linear transformation.\n",
    "        codebook_size (int): Number of unique tokens (characters) in the alphabet.\n",
    "        dropout_prob (float): Dropout probability for regularization.\n",
    "        ignore_token_ids (list[int]): Token ids to ignore when constructing sequences.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        representation_size: int,\n",
    "        hidden_size: int,\n",
    "        codebook_size: int,\n",
    "        dropout_prob: float = 0.1,\n",
    "        ignore_token_ids: list[int] = [0, 1, 2],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.representation_size = representation_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.codebook_size = codebook_size\n",
    "        self.ignore_token_ids = ignore_token_ids\n",
    "\n",
    "        self.dense = nn.Linear(representation_size, hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, codebook_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.eps = 1e-8  # Add epsilon for numerical stability\n",
    "\n",
    "        characters = list(\"ACDEFGHIKLMNPQRSTVWYacdefghiklmnpqrstvwy\")\n",
    "        self.characters = characters[:self.codebook_size]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def compute_shannon_entropy(self, logits):\n",
    "        # Convert logits to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        entropy = -torch.sum(\n",
    "            probs * torch.log(probs + self.eps), dim=-1\n",
    "        ) / torch.log(\n",
    "            torch.tensor(self.codebook_size, dtype=probs.dtype, device=probs.device)\n",
    "        )\n",
    "        return entropy\n",
    "\n",
    "    def to_sequences(\n",
    "        self,\n",
    "        input_ids,\n",
    "        embeddings,\n",
    "        attention_mask=None,\n",
    "        logits=None,\n",
    "        return_avg_entropy=False,\n",
    "        return_logits=False,\n",
    "        return_residue_entropy=False,\n",
    "    ):\n",
    "        if logits is None:\n",
    "            logits = self(embeddings)\n",
    "        \n",
    "        # Build a mask to ignore specified token ids\n",
    "        ignore_mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
    "        for token_id in self.ignore_token_ids:\n",
    "            ignore_mask &= (input_ids != token_id)\n",
    "        \n",
    "        # Get token predictions\n",
    "        predicted_indices = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        sequences = []\n",
    "        logits_list = []\n",
    "        residue_entropy_list = []\n",
    "        avg_entropy_list = []\n",
    "        \n",
    "        # Iterate over all sequences in the batch\n",
    "        for seq_idx, seq_logits, mask in zip(predicted_indices, logits, ignore_mask):\n",
    "            filtered_indices = seq_idx[mask]\n",
    "            filtered_logits = seq_logits[mask]\n",
    "            sequence = ''.join(self.characters[idx.item()] for idx in filtered_indices)\n",
    "            sequences.append(sequence)\n",
    "            logits_list.append(filtered_logits)\n",
    "            entropies = self.compute_shannon_entropy(filtered_logits)\n",
    "            residue_entropy_list.append(entropies)\n",
    "            avg_entropy_list.append(entropies.mean().item())\n",
    "        \n",
    "        # Decide on return type\n",
    "        if not (return_avg_entropy or return_logits or return_entropy):\n",
    "            return sequences\n",
    "        \n",
    "        result = {\"sequences\": sequences}\n",
    "        if return_avg_entropy:\n",
    "            result[\"avg_entropy\"] = avg_entropy_list\n",
    "        if return_residue_entropy:\n",
    "            result[\"residue_entropy\"] = residue_entropy_list\n",
    "        if return_logits:\n",
    "            result[\"logits\"] = logits_list\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9c2edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"representation_size\": 1280, \"hidden_size\": 1280, \"codebook_size\": 20, \"dropout_prob\": 0.1, \"ignore_token_ids\": [0, 1, 2]}\n",
    "model = Tea(**config)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2e2d79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move your model to GPU before loading weights\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c9e36ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_802849/3933074660.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(\"/scicore/home/schwede/durair0000/projects/alphabeta/AlphaBeta/data/final_ablations/entropy_01/all/checkpoints/entropy_01-all-epoch=9.ckpt\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.load(\"/scicore/home/schwede/durair0000/projects/alphabeta/AlphaBeta/data/final_ablations/entropy_01/all/checkpoints/entropy_01-all-epoch=9.ckpt\", map_location=device)\n",
    "weights[\"state_dict\"][\"model.lm_head.decoder.bias\"] = weights[\"state_dict\"][\"model.lm_head.decoder.bias\"] + weights[\"state_dict\"][\"model.lm_head.bias\"]\n",
    "state_dict = weights[\"state_dict\"]\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith(\"model.lm_head.\"):\n",
    "        new_key = k[len(\"model.lm_head.\"):]\n",
    "        if new_key == \"bias\":\n",
    "            continue\n",
    "        new_state_dict[new_key] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "model.load_state_dict(new_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ffc28f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequences': ['TPTTHPT', 'TTGHTTTT'],\n",
       " 'avg_entropy': [0.40297362208366394, 0.21876809000968933]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True) if torch.cuda.is_available() else None\n",
    "esm2 = AutoModel.from_pretrained(\n",
    "        \"facebook/esm2_t33_650M_UR50D\",\n",
    "        torch_dtype=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        add_pooling_layer=False,\n",
    "    )\n",
    "esm2.eval()\n",
    "sequence_examples = [\"PRTEINO\", \"SEQWENCE\"]\n",
    "sequence_examples = [\" \".join(list(re.sub(r\"[UZOBJ]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "ids = tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, padding=\"longest\")\n",
    "# device = next(model.parameters()).device\n",
    "input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "with torch.no_grad():\n",
    "    x = esm2(\n",
    "        input_ids=input_ids, attention_mask=attention_mask\n",
    "    ).last_hidden_state.to(device)\n",
    "    results = model.to_sequences(embeddings=x, input_ids=input_ids, return_avg_entropy=True)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f2fe5",
   "metadata": {},
   "source": [
    "# CHANGE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a5dabdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: Root=1-691210c4-1eb570ec760b65ea0184194f;9fc66cf8-b604-4276-a5f4-7eb7632d2d78)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"lorenzo-pantolini\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/alphabeta/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/mambaforge/envs/alphabeta/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtea\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# push to the hub\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlorenzo-pantolini/tea\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# reload\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AlphaBeta\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlorenzo-pantolini/tea\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/alphabeta/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/alphabeta/lib/python3.11/site-packages/huggingface_hub/hub_mixin.py:689\u001b[0m, in \u001b[0;36mModelHubMixin.push_to_hub\u001b[0;34m(self, repo_id, config, commit_message, private, token, branch, create_pr, allow_patterns, ignore_patterns, delete_patterns, model_card_kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;124;03mUpload model checkpoint to the Hub.\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;124;03m    The url of the commit of your model in the given repository.\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    688\u001b[0m api \u001b[38;5;241m=\u001b[39m HfApi(token\u001b[38;5;241m=\u001b[39mtoken)\n\u001b[0;32m--> 689\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrepo_id\n\u001b[1;32m    691\u001b[0m \u001b[38;5;66;03m# Push the files to the repo in a single commit\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SoftTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m tmp:\n",
      "File \u001b[0;32m~/mambaforge/envs/alphabeta/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/alphabeta/lib/python3.11/site-packages/huggingface_hub/hf_api.py:3544\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3542\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RepoUrl(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3543\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m HfHubHTTPError:\n\u001b[0;32m-> 3544\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m   3545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3546\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/alphabeta/lib/python3.11/site-packages/huggingface_hub/hf_api.py:3531\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3528\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   3530\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3531\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3532\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   3533\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[1;32m   3534\u001b[0m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/alphabeta/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:468\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    463\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     )\n\u001b[0;32m--> 468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m416\u001b[39m:\n\u001b[1;32m    471\u001b[0m     range_header \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: Root=1-691210c4-1eb570ec760b65ea0184194f;9fc66cf8-b604-4276-a5f4-7eb7632d2d78)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"lorenzo-pantolini\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions."
     ]
    }
   ],
   "source": [
    "# save locally\n",
    "model.save_pretrained(\"tea\")\n",
    "\n",
    "# push to the hub\n",
    "model.push_to_hub(\"lorenzo-pantolini/tea\")\n",
    "\n",
    "# reload\n",
    "model = AlphaBeta.from_pretrained(\"lorenzo-pantolini/tea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77348dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphabeta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
